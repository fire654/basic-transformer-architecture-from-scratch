{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e3b8681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adapters import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e2fa85",
   "metadata": {},
   "source": [
    "# 参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2edfd220",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=8000\n",
    "context_length=256\n",
    "d_model=768\n",
    "d_ff=3072\n",
    "theta=1000\n",
    "num_layers=6\n",
    "num_heads=12\n",
    "device=torch.device('cuda')\n",
    "dtype=torch.bfloat16\n",
    "batch_size=32\n",
    "lr=3e-4\n",
    "weight_decay=0.01\n",
    "epoch=1000\n",
    "grad_clip=1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192bd51a",
   "metadata": {},
   "source": [
    "# 读取文本并生成BPEtokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57933e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path='shakespear.txt'\n",
    "with open(text_path,'r',encoding='utf-8') as f:\n",
    "    text_example=f.read()\n",
    "vocab,merge=run_train_bpe(text_path,vocab_size,['<|endoftext|>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b52dc4",
   "metadata": {},
   "source": [
    "# 生成训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c62f26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_vocab_merges(vocab,merge,'BPE')\n",
    "tokenizer=get_tokenizer(vocab,merge,['<|endoftext|>'])\n",
    "np_lst=np.array(tokenizer.encode(text_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1026bf2",
   "metadata": {},
   "source": [
    "# 生成模型和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e186fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=TransformerLM(vocab_size,d_model,num_layers,num_heads,d_ff,context_length,theta,device,dtype)\n",
    "AdamW=get_adamw_cls()\n",
    "optimizer=AdamW(params=model.parameters(),weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fca9ce",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdd7ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 100 times loss: 2.75\n",
      "train 200 times loss: 2.328125\n",
      "train 300 times loss: 2.0\n",
      "train 400 times loss: 1.875\n",
      "train 500 times loss: 1.8515625\n",
      "train 600 times loss: 1.796875\n",
      "train 700 times loss: 1.5703125\n",
      "train 800 times loss: 1.46875\n",
      "train 900 times loss: 1.2734375\n",
      "train 1000 times loss: 1.3984375\n"
     ]
    }
   ],
   "source": [
    "#faster version\n",
    "model.train()\n",
    "for i in range(epoch):\n",
    "    input,label=run_get_batch(np_lst,batch_size,context_length,device)\n",
    "    optimizer.zero_grad()\n",
    "    output=model(input)\n",
    "    loss=run_cross_entropy(output.reshape(-1,vocab_size),label.reshape(-1))\n",
    "    loss.backward()\n",
    "    run_gradient_clipping(model.parameters(),grad_clip)\n",
    "    optimizer.step()\n",
    "    if (i+1)%100==0:\n",
    "        print(f'train {i+1} times loss: {loss}')\n",
    "        run_save_checkpoint(model,optimizer,(i+1)//100,'state_dict.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618f8223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100 | Loss: 1.1484 | Current LR: 0.000030\n",
      "Iteration 200 | Loss: 1.0781 | Current LR: 0.000060\n",
      "Iteration 300 | Loss: 1.2891 | Current LR: 0.000090\n",
      "Iteration 400 | Loss: 1.0547 | Current LR: 0.000120\n",
      "Iteration 500 | Loss: 1.0156 | Current LR: 0.000150\n",
      "Iteration 600 | Loss: 0.8281 | Current LR: 0.000180\n",
      "Iteration 700 | Loss: 0.7422 | Current LR: 0.000210\n",
      "Iteration 800 | Loss: 0.5977 | Current LR: 0.000240\n",
      "Iteration 900 | Loss: 0.5156 | Current LR: 0.000270\n",
      "Iteration 1000 | Loss: 0.4629 | Current LR: 0.000300\n",
      "Iteration 1100 | Loss: 0.3652 | Current LR: 0.000300\n",
      "Iteration 1200 | Loss: 0.3555 | Current LR: 0.000300\n",
      "Iteration 1300 | Loss: 0.2832 | Current LR: 0.000300\n",
      "Iteration 1400 | Loss: 0.2295 | Current LR: 0.000300\n",
      "Iteration 1500 | Loss: 0.2119 | Current LR: 0.000300\n",
      "Iteration 1600 | Loss: 0.2168 | Current LR: 0.000300\n",
      "Iteration 1700 | Loss: 0.1943 | Current LR: 0.000300\n",
      "Iteration 1800 | Loss: 0.1582 | Current LR: 0.000300\n",
      "Iteration 1900 | Loss: 0.1582 | Current LR: 0.000300\n",
      "Iteration 2000 | Loss: 0.1514 | Current LR: 0.000300\n",
      "Iteration 2100 | Loss: 0.1553 | Current LR: 0.000300\n",
      "Iteration 2200 | Loss: 0.1426 | Current LR: 0.000300\n",
      "Iteration 2300 | Loss: 0.1475 | Current LR: 0.000299\n",
      "Iteration 2400 | Loss: 0.1406 | Current LR: 0.000299\n",
      "Iteration 2500 | Loss: 0.1416 | Current LR: 0.000299\n",
      "Iteration 2600 | Loss: 0.1328 | Current LR: 0.000299\n",
      "Iteration 2700 | Loss: 0.1299 | Current LR: 0.000299\n",
      "Iteration 2800 | Loss: 0.1230 | Current LR: 0.000299\n",
      "Iteration 2900 | Loss: 0.1226 | Current LR: 0.000299\n",
      "Iteration 3000 | Loss: 0.1118 | Current LR: 0.000299\n",
      "Iteration 3100 | Loss: 0.1211 | Current LR: 0.000299\n",
      "Iteration 3200 | Loss: 0.1211 | Current LR: 0.000299\n",
      "Iteration 3300 | Loss: 0.1196 | Current LR: 0.000298\n",
      "Iteration 3400 | Loss: 0.1118 | Current LR: 0.000298\n",
      "Iteration 3500 | Loss: 0.1162 | Current LR: 0.000298\n",
      "Iteration 3600 | Loss: 0.1167 | Current LR: 0.000298\n",
      "Iteration 3700 | Loss: 0.1123 | Current LR: 0.000298\n",
      "Iteration 3800 | Loss: 0.1133 | Current LR: 0.000298\n",
      "Iteration 3900 | Loss: 0.1138 | Current LR: 0.000298\n",
      "Iteration 4000 | Loss: 0.1025 | Current LR: 0.000297\n",
      "Iteration 4100 | Loss: 0.1060 | Current LR: 0.000297\n",
      "Iteration 4200 | Loss: 0.1006 | Current LR: 0.000297\n",
      "Iteration 4300 | Loss: 0.1006 | Current LR: 0.000297\n",
      "Iteration 4400 | Loss: 0.0991 | Current LR: 0.000297\n",
      "Iteration 4500 | Loss: 0.0952 | Current LR: 0.000296\n",
      "Iteration 4600 | Loss: 0.1001 | Current LR: 0.000296\n",
      "Iteration 4700 | Loss: 0.0933 | Current LR: 0.000296\n",
      "Iteration 4800 | Loss: 0.1133 | Current LR: 0.000296\n",
      "Iteration 4900 | Loss: 0.1060 | Current LR: 0.000295\n",
      "Iteration 5000 | Loss: 0.1006 | Current LR: 0.000295\n",
      "Iteration 5100 | Loss: 0.1035 | Current LR: 0.000295\n",
      "Iteration 5200 | Loss: 0.1040 | Current LR: 0.000295\n",
      "Iteration 5300 | Loss: 0.0957 | Current LR: 0.000295\n",
      "Iteration 5400 | Loss: 0.0977 | Current LR: 0.000294\n",
      "Iteration 5500 | Loss: 0.0962 | Current LR: 0.000294\n",
      "Iteration 5600 | Loss: 0.0903 | Current LR: 0.000294\n",
      "Iteration 5700 | Loss: 0.0947 | Current LR: 0.000293\n",
      "Iteration 5800 | Loss: 0.0933 | Current LR: 0.000293\n",
      "Iteration 5900 | Loss: 0.0854 | Current LR: 0.000293\n",
      "Iteration 6000 | Loss: 0.0928 | Current LR: 0.000293\n",
      "Iteration 6100 | Loss: 0.0913 | Current LR: 0.000292\n",
      "Iteration 6200 | Loss: 0.0928 | Current LR: 0.000292\n",
      "Iteration 6300 | Loss: 0.0864 | Current LR: 0.000292\n",
      "Iteration 6400 | Loss: 0.1006 | Current LR: 0.000291\n",
      "Iteration 6500 | Loss: 0.0918 | Current LR: 0.000291\n",
      "Iteration 6600 | Loss: 0.0864 | Current LR: 0.000291\n",
      "Iteration 6700 | Loss: 0.0874 | Current LR: 0.000290\n",
      "Iteration 6800 | Loss: 0.0889 | Current LR: 0.000290\n",
      "Iteration 6900 | Loss: 0.0884 | Current LR: 0.000290\n",
      "Iteration 7000 | Loss: 0.0864 | Current LR: 0.000289\n",
      "Iteration 7100 | Loss: 0.0840 | Current LR: 0.000289\n",
      "Iteration 7200 | Loss: 0.0884 | Current LR: 0.000289\n",
      "Iteration 7300 | Loss: 0.0869 | Current LR: 0.000288\n",
      "Iteration 7400 | Loss: 0.0874 | Current LR: 0.000288\n",
      "Iteration 7500 | Loss: 0.0791 | Current LR: 0.000288\n",
      "Iteration 7600 | Loss: 0.0884 | Current LR: 0.000287\n",
      "Iteration 7700 | Loss: 0.0767 | Current LR: 0.000287\n",
      "Iteration 7800 | Loss: 0.0835 | Current LR: 0.000286\n",
      "Iteration 7900 | Loss: 0.0801 | Current LR: 0.000286\n",
      "Iteration 8000 | Loss: 0.0864 | Current LR: 0.000286\n",
      "Iteration 8100 | Loss: 0.0854 | Current LR: 0.000285\n",
      "Iteration 8200 | Loss: 0.0776 | Current LR: 0.000285\n",
      "Iteration 8300 | Loss: 0.0820 | Current LR: 0.000284\n",
      "Iteration 8400 | Loss: 0.0830 | Current LR: 0.000284\n",
      "Iteration 8500 | Loss: 0.0820 | Current LR: 0.000284\n",
      "Iteration 8600 | Loss: 0.0898 | Current LR: 0.000283\n",
      "Iteration 8700 | Loss: 0.0781 | Current LR: 0.000283\n",
      "Iteration 8800 | Loss: 0.0845 | Current LR: 0.000282\n",
      "Iteration 8900 | Loss: 0.0815 | Current LR: 0.000282\n",
      "Iteration 9000 | Loss: 0.0767 | Current LR: 0.000281\n",
      "Iteration 9100 | Loss: 0.0688 | Current LR: 0.000281\n",
      "Iteration 9200 | Loss: 0.0776 | Current LR: 0.000280\n",
      "Iteration 9300 | Loss: 0.0757 | Current LR: 0.000280\n",
      "Iteration 9400 | Loss: 0.0762 | Current LR: 0.000279\n",
      "Iteration 9500 | Loss: 0.0771 | Current LR: 0.000279\n",
      "Iteration 9600 | Loss: 0.0776 | Current LR: 0.000279\n",
      "Iteration 9700 | Loss: 0.0767 | Current LR: 0.000278\n",
      "Iteration 9800 | Loss: 0.0781 | Current LR: 0.000278\n",
      "Iteration 9900 | Loss: 0.0845 | Current LR: 0.000277\n",
      "Iteration 10000 | Loss: 0.0776 | Current LR: 0.000277\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m run_cross_entropy(output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size), label\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     23\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 24\u001b[0m \u001b[43mrun_gradient_clipping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_clip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\sanak\\Desktop\\basic-transformer-architecture-from-scratch-main\\adapters.py:684\u001b[0m, in \u001b[0;36mrun_gradient_clipping\u001b[1;34m(parameters, max_l2_norm)\u001b[0m\n\u001b[0;32m    681\u001b[0m all_grads \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(grads)\n\u001b[0;32m    682\u001b[0m total_l2_norm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(all_grads, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtotal_l2_norm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_l2_norm\u001b[49m:\n\u001b[0;32m    685\u001b[0m     scale_factor \u001b[38;5;241m=\u001b[39m max_l2_norm \u001b[38;5;241m/\u001b[39m (total_l2_norm \u001b[38;5;241m+\u001b[39m eps)\n\u001b[0;32m    686\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#better version\n",
    "model.train()\n",
    "max_learning_rate = 3e-4    # 最大学习率（适配d_model=768的模型）\n",
    "min_learning_rate = 1e-5    # 最小学习率（防止学习率过低导致停止更新）\n",
    "warmup_iters = 1000         # 预热迭代数（前1000步线性升温）\n",
    "cosine_cycle_iters = 50000  # 余弦退火总迭代数（预热后到50000步完成退火）\n",
    "grad_clip = 1.0             # 梯度裁剪阈值\n",
    "epoch = 50000\n",
    "for i in range(epoch):\n",
    "    input, label = run_get_batch(np_lst, batch_size, context_length, device)\n",
    "    current_lr = run_get_lr_cosine_schedule(\n",
    "        it=i,\n",
    "        max_learning_rate=max_learning_rate,\n",
    "        min_learning_rate=min_learning_rate,\n",
    "        warmup_iters=warmup_iters,\n",
    "        cosine_cycle_iters=cosine_cycle_iters\n",
    "    )\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = current_lr\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input)\n",
    "    loss = run_cross_entropy(output.reshape(-1, vocab_size), label.reshape(-1))\n",
    "    loss.backward()\n",
    "    run_gradient_clipping(model.parameters(), grad_clip)\n",
    "    optimizer.step()\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f'Iteration {i+1} | Loss: {loss.item():.4f} | Current LR: {current_lr:.6f}')\n",
    "        run_save_checkpoint(model, optimizer, (i+1)//100, 'state_dict.pt')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
